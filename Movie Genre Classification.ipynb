{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import neuralcoref\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#7DaysLater\" (2013)</td>\n",
       "      <td>#7dayslater is an interactive comedy series f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n",
       "      <td>With just one week left in the workshops, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n",
       "      <td>All of the women start making strides towards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n",
       "      <td>All five of these women are independent and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n",
       "      <td>Despite having gone through a life changing p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                               \"#7DaysLater\" (2013)   \n",
       "1       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n",
       "2  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n",
       "3      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n",
       "4     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n",
       "\n",
       "                                                plot  \n",
       "0   #7dayslater is an interactive comedy series f...  \n",
       "1   With just one week left in the workshops, the...  \n",
       "2   All of the women start making strides towards...  \n",
       "3   All five of these women are independent and s...  \n",
       "4   Despite having gone through a life changing p...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baca file dataset\n",
    "df = pd.read_csv('movies_genres.csv', sep='\\t', nrows=100)\n",
    "\n",
    "# aku kepikirannya bikin model buat setiap genre, jadi labelnya dipisah\n",
    "label_action = df['Action']\n",
    "label_adult = df['Adult']\n",
    "label_adventure = df['Adventure']\n",
    "label_animation = df['Animation']\n",
    "label_biography = df['Biography']\n",
    "label_comedy = df['Comedy']\n",
    "label_crime = df['Crime']\n",
    "label_documentary = df['Documentary']\n",
    "label_drama = df['Drama']\n",
    "label_family = df['Family']\n",
    "label_fantasy = df['Fantasy']\n",
    "label_gameshow = df['Game-Show']\n",
    "label_history = df['History']\n",
    "label_horror = df['Horror']\n",
    "label_lifestyle = df['Lifestyle']\n",
    "label_music = df['Music']\n",
    "label_musical = df['Musical']\n",
    "label_mystery = df['Mystery']\n",
    "label_news = df['News']\n",
    "label_reality = df['Reality-TV']\n",
    "label_romance = df['Romance']\n",
    "label_scifi = df['Sci-Fi']\n",
    "label_short = df['Short']\n",
    "label_sport = df['Sport']\n",
    "label_talkshow = df['Talk-Show']\n",
    "label_thriller = df['Thriller']\n",
    "label_war = df['War']\n",
    "label_western = df['Western']\n",
    "\n",
    "df = df.drop(columns=['Action', 'Adult', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Lifestyle', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi', 'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "\n",
      "Walt Disney: [\n",
      "\n",
      "Walt Disney, Disney, Disney, Disney, Disney, Disney, Disney, Disney, Disney himself, Disney, Disney], He: [He, his, His, his], Charles Mintz: [Charles Mintz, Mintz, his], The Walt Disney Company: [The Walt Disney Company, this company], his: [his, him, He]]\n",
      "\n",
      "\n",
      "Walt Disney was born on December 5, 1901. \n",
      "\n",
      "Walt Disney became one of the best-known motion picture producers in the world. He is particularly noted for being a film producer and a popular showman, as well as an innovator in animation and theme park design.\n",
      "\n",
      "\n",
      "\n",
      "Walt Disney is famous for He contributions in the field of entertainment during the 20th century. He first success was through the series, Oswald the Lucky Rabbit which was created by the \n",
      "\n",
      "Walt Disney studio for Charles Mintz of Universal Studios. When \n",
      "\n",
      "Walt Disney asked for a larger budget for He popular Oswald series, Charles Mintz refused and \n",
      "\n",
      "Walt Disney had to quit. Later, \n",
      "\n",
      "Walt Disney and Charles Mintz brother Roy O. Disney started from scratch and co-founded Walt Disney Productions, now known as The Walt Disney Company. Today, The Walt Disney Company has annual revenues of approximately U.S. $35 billion. This success is largely due to a number of the world's most famous fictional characters he and his staff created including Mickey Mouse, a character for which \n",
      "\n",
      "Walt Disney was the original voice.\n",
      "\n",
      "\n",
      "\n",
      "Walt Disney won 26 Academy Awards out of 59 nominations, including a record four in one year, giving his more awards and nominations than any other individual. his is also the namesake for Disneyland and Walt Disney World Resort theme parks in the United States, as well as the international resorts in Japan, France, and China.\n",
      "\n",
      "\n",
      "\n",
      "Walt Disney died of lung cancer in Burbank, California, on December 15, 1966. The following year, construction began on Walt Disney World Resort in Florida. His brother Roy Disney inaugurated The Magic Kingdom on October 1, 1971.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coreference resolution\n",
    "\n",
    "# experiment\n",
    "text = \"\"\"\n",
    "\n",
    "Walt Disney was born on December 5, 1901. Disney became one of the best-known motion picture producers in the world. He is particularly noted for being a film producer and a popular showman, as well as an innovator in animation and theme park design.\n",
    "\n",
    "Disney is famous for his contributions in the field of entertainment during the 20th century. His first success was through the series, Oswald the Lucky Rabbit which was created by the Disney studio for Charles Mintz of Universal Studios. When Disney asked for a larger budget for his popular Oswald series, Mintz refused and Disney had to quit. Later, Disney and his brother Roy O. Disney started from scratch and co-founded Walt Disney Productions, now known as The Walt Disney Company. Today, this company has annual revenues of approximately U.S. $35 billion. This success is largely due to a number of the world's most famous fictional characters he and his staff created including Mickey Mouse, a character for which Disney himself was the original voice.\n",
    "\n",
    "Disney won 26 Academy Awards out of 59 nominations, including a record four in one year, giving him more awards and nominations than any other individual. He is also the namesake for Disneyland and Walt Disney World Resort theme parks in the United States, as well as the international resorts in Japan, France, and China.\n",
    "\n",
    "Disney died of lung cancer in Burbank, California, on December 15, 1966. The following year, construction began on Walt Disney World Resort in Florida. His brother Roy Disney inaugurated The Magic Kingdom on October 1, 1971.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp, greedyness=0.45, max_dist=40, max_dist_match=500)\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc._.coref_clusters)\n",
    "print(doc._.coref_resolved)\n",
    "\n",
    "# pre-processing\n",
    "def coref_resolution(dataframe, nlp):\n",
    "    for i in range(0, len(dataframe)):\n",
    "        doc = nlp(dataframe['plot'][i])\n",
    "        dataframe.at[i, 'plot'] = doc._.coref_resolved\n",
    "    \n",
    "coref_resolution(df, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    text: kalimat yang bakal ditokenize\n",
    "    contoh: `token = tokenize(token_text)`\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    result = text.split(' ')\n",
    "    return list(filter(lambda word: word != '', result))\n",
    "\n",
    "# token = tokenize(token_text)\n",
    "# print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #7dayslater is an interactive comedy series featuring an ensemble cast of YouTube celebrities. Each week the audience writes the brief via social media for an all-new episode featuring a well-known guest-star. Seven days later that week's episode premieres on TV and across multiple platforms.  With just one week left in the workshops, the women consider the idea of \"The One.\" The ladies are stunned when Jahmil finally comes to a decision about Bentley and if Bentley's the one for her. Jack challenges Tennesha to express her feelings of love towards Errol, but can her put her out there and face possible rejection?  All of the women start making strides towards finding  All of the women own version of a happy ending. Tennesha and Errol decide to become exclusive, but Laree just isn't ready to tell Karl Laree loves Karl, even though Karl has expressed that sentiment to Laree. Cynthia finds it hard to venture out on Cynthia own after Cynthia tumultuous separation, and Monet finally finds peace in Cynthia divorce when Cynthia decides to throw a 'freedom' party to celebrate!  All five of these women are independent and strong willed, and because of this, these women've faced strife in these women relationships, past and present. On Jamil's second date with Nneka, Nneka shares that she wants a committed relationship, leaving Jahmil in a difficult position. Jack drops a bomb on Monet in their one on one, which makes she realize that she has been looking at she dating life all wrong. Tennesha is ready to introduce the ladies to Errol at Errol housewarming party, but Errol nervousness leads to Errol controlling nature coming out again. Can Errol reign it in before Errol drives Errol away?  Despite having gone through a life changing process in the past ten weeks, the women still have issues to resolve before the week is up in order for the women to get the women love lives in check. Monet realizes Monet has one final piece to let go of.  #CAKE is a hour-long serial narrative comedy about a manhunt for a high priced assassin who murders for digital currency (Bitcoin) , the detectives who track him, and a enthused entrepreneur who runs a bitcoin mining operation. In a perfect world do you want money or power? You want the power to create your own money.  When you're married to a sports nut, sometimes a sports nut helps to know a little about their favorite teams. Like, for instance, what sport the team plays. So Nicole sets out to learn everything there is to know about every single sport...in one sitting.  #Elmira follows the story of a bunch of strangers who all respond to the same CraigsList ad \"Looking for roommates to share rent.\" Upon their arrival, the apartment is a lot smaller than anticipated, and so the flat is overpopulated. The fictional apartment building, \"The Elmira,\" located on the fictional street, \"Elmira,\" resides in the City of Elmira. the apartment is rented to the gang by a campy, gay, married couple, who always seem to show up unannounced at the most inopportune times. To make matters worse the apartment sits above a noisy bar. A drag bar called \"Drag-Hag,\" which is the center of confusion to some of the characters.  Friend Me. Follow Me. Like Me. Fall for Me. #Hashtag follows the love lives of two technology-obsessed best friends in Chicago. From Instagram seduction to inappropriate selfies, Twitter over sharing to OKCupid dating, Liv and Skylar are about to learn the real-life consequences of over-indulgence in the virtual world.  #LawstinWoods follows the story of 6 strangers who were taken from their lives and placed in woods called the \"Lawstin Woods.\" The mysterious name of the woods happens to parallel their own misfortune, which is that their are all lost in woods. While trapped in the mystical forest they struggle to get answers, while they fight to stay alive. All the while a small group of people who claim to be residents of the Lawstin Woods taunt those who're lost, but never give the gang answers. Instead those who're lost appear to simply toy with those who're lost. \n",
      "['7dayslater', 'be', 'an', 'interactive', 'comedy', 'series', 'feature', 'an', 'ensemble', 'cast', 'of', 'YouTube', 'celebrity', 'each', 'week', 'the', 'audience', 'write', 'the', 'brief', 'via', 'social', 'medium', 'for', 'an', 'all', 'new', 'episode', 'feature', 'well', 'know', 'guest', 'star', 'seven', 'day', 'later', 'that', 'week', \"'s\", 'episode', 'premiere', 'on', 'tv', 'and', 'across', 'multiple', 'platform', 'with', 'just', 'one', 'week', 'leave', 'in', 'the', 'workshop', 'the', 'woman', 'consider', 'the', 'idea', 'of', 'the', 'one', 'the', 'lady', 'be', 'stun', 'when', 'Jahmil', 'finally', 'come', 'to', 'decision', 'about', 'Bentley', 'and', 'if', 'Bentley', \"'s\", 'the', 'one', 'for', 'Jack', 'challenge', 'Tennesha', 'to', 'express', 'feeling', 'of', 'love', 'towards', 'Errol', 'but', 'can', 'put', 'out', 'there', 'and', 'face', 'possible', 'rejection', 'all', 'of', 'the', 'woman', 'start', 'make', 'stride', 'towards', 'find', 'all', 'of', 'the', 'woman', 'own', 'version', 'of', 'happy', 'ending', 'Tennesha', 'and', 'Errol', 'decide', 'to', 'become', 'exclusive', 'but', 'Laree', 'just', 'be', 'not', 'ready', 'to', 'tell', 'Karl', 'Laree', 'love', 'Karl', 'even', 'though', 'Karl', 'have', 'express', 'that', 'sentiment', 'to', 'Laree', 'Cynthia', 'find', 'hard', 'to', 'venture', 'out', 'on', 'Cynthia', 'own', 'after', 'Cynthia', 'tumultuous', 'separation', 'and', 'Monet', 'finally', 'find', 'peace', 'in', 'Cynthia', 'divorce', 'when', 'Cynthia', 'decide', 'to', 'throw', 'freedom', 'party', 'to', 'celebrate', 'all', 'five', 'of', 'these', 'woman', 'be', 'independent', 'and', 'strong', 'willed', 'and', 'because', 'of', 'this', 'these', \"women've\", 'face', 'strife', 'in', 'these', 'woman', 'relationship', 'past', 'and', 'present', 'on', 'Jamil', \"'s\", 'second', 'date', 'with', 'Nneka', 'Nneka', 'share', 'that', 'want', 'committed', 'relationship', 'leave', 'Jahmil', 'in', 'difficult', 'position', 'Jack', 'drop', 'bomb', 'on', 'Monet', 'in', 'one', 'on', 'one', 'which', 'make', 'realize', 'that', 'have', 'be', 'look', 'at', 'date', 'life', 'all', 'wrong', 'Tennesha', 'be', 'ready', 'to', 'introduce', 'the', 'lady', 'to', 'Errol', 'at', 'Errol', 'housewarme', 'party', 'but', 'Errol', 'nervousness', 'lead', 'to', 'Errol', 'control', 'nature', 'come', 'out', 'again', 'Can', 'Errol', 'reign', 'in', 'before', 'Errol', 'drive', 'Errol', 'away', 'despite', 'have', 'go', 'through', 'life', 'change', 'process', 'in', 'the', 'past', 'ten', 'week', 'the', 'woman', 'still', 'have', 'issue', 'to', 'resolve', 'before', 'the', 'week', 'be', 'up', 'in', 'order', 'for', 'the', 'woman', 'to', 'get', 'the', 'woman', 'love', 'live', 'in', 'check', 'Monet', 'realize', 'Monet', 'have', 'one', 'final', 'piece', 'to', 'let', 'go', 'of', 'cake', 'be', 'hour', 'long', 'serial', 'narrative', 'comedy', 'about', 'manhunt', 'for', 'high', 'price', 'assassin', 'who', 'murder', 'for', 'digital', 'currency', 'Bitcoin', 'the', 'detective', 'who', 'track', 'and', 'enthused', 'entrepreneur', 'who', 'run', 'bitcoin', 'mining', 'operation', 'in', 'perfect', 'world', 'do', 'want', 'money', 'or', 'power', 'want', 'the', 'power', 'to', 'create', 'own', 'money', 'when', 'be', 'married', 'to', 'sport', 'nut', 'sometimes', 'sport', 'nut', 'help', 'to', 'know', 'little', 'about', 'favorite', 'team', 'like', 'for', 'instance', 'what', 'sport', 'the', 'team', 'play', 'so', 'Nicole', 'set', 'out', 'to', 'learn', 'everything', 'there', 'be', 'to', 'know', 'about', 'every', 'single', 'sport', '...', 'in', 'one', 'sit', 'Elmira', 'follow', 'the', 'story', 'of', 'bunch', 'of', 'stranger', 'who', 'all', 'respond', 'to', 'the', 'same', 'CraigsList', 'ad', 'look', 'for', 'roommate', 'to', 'share', 'rent', 'upon', 'arrival', 'the', 'apartment', 'be', 'lot', 'small', 'than', 'anticipate', 'and', 'so', 'the', 'flat', 'be', 'overpopulate', 'the', 'fictional', 'apartment', 'building', 'the', 'Elmira', 'locate', 'on', 'the', 'fictional', 'street', 'Elmira', 'reside', 'in', 'the', 'City', 'of', 'Elmira', 'the', 'apartment', 'be', 'rent', 'to', 'the', 'gang', 'by', 'campy', 'gay', 'married', 'couple', 'who', 'always', 'seem', 'to', 'show', 'up', 'unannounced', 'at', 'the', 'most', 'inopportune', 'time', 'to', 'make', 'matter', 'bad', 'the', 'apartment', 'sit', 'above', 'noisy', 'bar', 'drag', 'bar', 'call', 'Drag', 'hag', 'which', 'be', 'the', 'center', 'of', 'confusion', 'to', 'some', 'of', 'the', 'character', 'friend', 'follow', 'like', 'fall', 'for', 'Hashtag', 'follow', 'the', 'love', 'live', 'of', 'two', 'technology', 'obsess', 'good', 'friend', 'in', 'Chicago', 'from', 'Instagram', 'seduction', 'to', 'inappropriate', 'selfie', 'Twitter', 'over', 'share', 'to', 'okcupid', 'dating', 'Liv', 'and', 'Skylar', 'be', 'about', 'to', 'learn', 'the', 'real', 'life', 'consequence', 'of', 'over', 'indulgence', 'in', 'the', 'virtual', 'world', 'LawstinWoods', 'follow', 'the', 'story', 'of', 'stranger', 'who', 'be', 'take', 'from', 'life', 'and', 'place', 'in', 'wood', 'call', 'the', 'Lawstin', 'Woods', 'the', 'mysterious', 'name', 'of', 'the', 'wood', 'happen', 'to', 'parallel', 'own', 'misfortune', 'which', 'be', 'that', 'be', 'all', 'lose', 'in', 'wood', 'while', 'trap', 'in', 'the', 'mystical', 'forest', 'struggle', 'to', 'get', 'answer', 'while', 'fight', 'to', 'stay', 'alive', 'all', 'the', 'while', 'small', 'group', 'of', 'people', 'who', 'claim', 'to', 'be', 'resident', 'of', 'the', 'Lawstin', 'Woods', 'taunt', 'those', 'who', 'be', 'lose', 'but', 'never', 'give', 'the', 'gang', 'answer', 'instead', 'those', 'who', 'be', 'lose', 'appear', 'to', 'simply', 'toy', 'with', 'those', 'who', 'be', 'lose']\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "# # untuk coba-coba aja\n",
    "# lem_text = \"\"\n",
    "# for idx in range(10):\n",
    "#     lem_text += \"{} \".format(df[['plot']].iloc[idx]['plot'])\n",
    "\n",
    "\n",
    "def lemmatization(sentence):\n",
    "    \"\"\"\n",
    "    sentence: kalimat yang akan di-lemma, belum \n",
    "    \"\"\"\n",
    "    sentence = nlp(sentence)\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        if len(word.lemma_) != 1 and word.lemma_ != \"-PRON-\":\n",
    "            result.append(word.lemma_)\n",
    "    return result\n",
    "\n",
    "# print(lem_text)\n",
    "# lem_result = lemmatization(lem_text)\n",
    "# print(lem_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtreebank\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('treebank')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/treebank/combined\u001b[0m\n\n  Searched in:\n    - '/Users/shevalda/nltk_data'\n    - '/Users/shevalda/anaconda3/nltk_data'\n    - '/Users/shevalda/anaconda3/share/nltk_data'\n    - '/Users/shevalda/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtreebank\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('treebank')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/treebank.zip/treebank/combined/\u001b[0m\n\n  Searched in:\n    - '/Users/shevalda/nltk_data'\n    - '/Users/shevalda/anaconda3/nltk_data'\n    - '/Users/shevalda/anaconda3/share/nltk_data'\n    - '/Users/shevalda/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-169b407616c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtagged_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Split the dataset for training and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtreebank\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('treebank')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/treebank/combined\u001b[0m\n\n  Searched in:\n    - '/Users/shevalda/nltk_data'\n    - '/Users/shevalda/anaconda3/nltk_data'\n    - '/Users/shevalda/anaconda3/share/nltk_data'\n    - '/Users/shevalda/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# pos tag\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "# Split the dataset for training and testing\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "print(len(training_sentences))   # 2935\n",
    "print(len(test_sentences))    # 979\n",
    " \n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X, y = transform_to_dataset(training_sentences)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X[:10000], y[:10000])   # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
    " \n",
    "print('Training completed')\n",
    " \n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    " \n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ner\n",
    "for column in df[['plot']]:\n",
    "   # Select column contents by column name using [] operator\n",
    "   columnSeriesObj = df[column]\n",
    "   print(columnSeriesObj.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import unicodedata\n",
    " \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
    " \n",
    "# Defining Constants\n",
    " \n",
    "# Maximum length of text sentences\n",
    "MAXLEN = 180\n",
    "# Number of LSTM units\n",
    "LSTM_N = 150\n",
    "# batch size\n",
    "BS=48\n",
    "\n",
    "data = pd.read_csv(\"ner_train.csv\", encoding=\"latin1\")\n",
    "test_data = pd.read_csv(\"ner_test.csv\", encoding=\"latin1\")\n",
    "\n",
    "print(\"Number of uniques docs, sentences and words in Training set:\\n\",data.nunique())\n",
    "print(\"\\nNumber of uniques docs, sentences and words in Test set:\\n\",test_data.nunique())\n",
    " \n",
    "# Creating a vocabulary\n",
    "words = list(set(data[\"Word\"].append(test_data[\"Word\"]).values))\n",
    "words.append(\"ENDPAD\")\n",
    " \n",
    "# Converting greek characters to ASCII characters eg. 'naïve café' to 'naive cafe'\n",
    "words = [unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore') for w in words]\n",
    "n_words = len(words)\n",
    "print(\"\\nLength of vocabulary = \",n_words)\n",
    " \n",
    "tags = list(set(data[\"tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(\"\\nnumber of tags = \",n_tags)\n",
    " \n",
    "# Creating words to indices dictionary.\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "# Creating tags to indices dictionary.\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "def get_tagged_sentences(data):\n",
    "    agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"tag\"].values.tolist())]\n",
    "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
    "    sentences = [s for s in grouped]\n",
    "    return sentences\n",
    " \n",
    "def get_test_sentences(data):\n",
    "    agg_func = lambda s: [w for w in s[\"Word\"].values.tolist()]\n",
    "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
    "    sentences = [s for s in grouped]\n",
    "    return sentences\n",
    "\n",
    "# Getting training sentences in a list\n",
    "sentences = get_tagged_sentences(data)\n",
    "print(\"First 2 sentences in a word list format:\\n\",sentences[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting words to indices for test sentences (Features)\n",
    "# Converting greek characters to ASCII characters in train set eg. 'naïve café' to 'naive cafe'\n",
    "X = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).\n",
    "encode('ascii','ignore')] for w in s] for s in sentences]\n",
    " \n",
    "# Converting words to indices for test sentences (Features)\n",
    "# Converting greek characters to ASCII characters in test-set eg. 'naïve café' to 'naive cafe'\n",
    "# X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
    "# encode('ascii','ignore')] for w in s] for s in test_sentences]\n",
    " \n",
    "'''\n",
    "Padding train and test sentences to 180 words.\n",
    "Sentences of length greater than 180 words are truncated.\n",
    "Sentences of length less than 180 words are padded with a high value.\n",
    "'''\n",
    "X = pad_sequences(maxlen=MAXLEN, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "# X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)\n",
    " \n",
    "# Converting tags to indices for test sentences (labels)\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "# Padding tag labels to 180 words.\n",
    "y = pad_sequences(maxlen=MAXLEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    " \n",
    "# Making labels in one hot encoded form for DL model\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "\n",
    "\n",
    "# 180 dimensional word indices as input\n",
    "input = Input(shape=(MAXLEN,))\n",
    " \n",
    "# Embedding layer of same length output (180 dim embedding will be generated)\n",
    "model = Embedding(input_dim=n_words, output_dim=MAXLEN, input_length=MAXLEN)(input)\n",
    " \n",
    "# Adding dropout layer\n",
    "model = Dropout(0.2)(model)\n",
    " \n",
    "# Bidirectional LSTM to learn from both forward as well as backward context\n",
    "model = Bidirectional(LSTM(units=LSTM_N, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    " \n",
    "# Adding a TimeDistributedDense, to applying a Dense layer on each 180 timesteps\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model) # softmax output layer\n",
    "model = Model(input, out)\n",
    " \n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, np.array(y), batch_size=BS, epochs=1, validation_split=0.5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_data)\n",
    "# print()\n",
    "test_sentences = get_test_sentences(test_data)\n",
    "# print(type(test_sentences))\n",
    "test_sentences1 = [['woody','lost','in','the','woods'], ['aliens','invading','earth']]\n",
    "# test_sentences2 = [['i', 'need', 'that', 'movie', 'which', 'involves', 'aliens', 'invading', 'earth', 'in', 'a', 'particular', 'united', 'states', 'place', 'in', 'california'], ['what', 'soviet', 'science', 'fiction', 'classic', 'about', 'a', 'mysterious', 'planet', 'was', 'later', 'remade', 'by', 'steven', 'soderbergh', 'and', 'george', 'clooney']]\n",
    "# print(len(test_sentences1))\n",
    "# print(len(test_sentences2))\n",
    "print(\"First 2 sentences in a word list format:\\n\",test_sentences1[0:2])\n",
    "\n",
    "X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
    "encode('ascii','ignore')] for w in s] for s in test_sentences1]\n",
    "\n",
    "X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on trained model\n",
    "pred = model.predict(X_test)\n",
    "print(\"Predicted Probabilities on Test Set:\\n\",pred.shape)\n",
    "# taking tag class with maximum probability\n",
    "pred_index = np.argmax(pred, axis=-1)\n",
    "print(\"Predicted tag indices: \\n\",pred_index.shape)\n",
    "\n",
    "# Flatten both the features and predicted tags for submission\n",
    "ids,tagids = X_test.flatten().tolist(), pred_index.flatten().tolist()\n",
    "\n",
    "# converting each word indices back to words\n",
    "words_test = [words[ind].decode('utf-8') for ind in ids]\n",
    "# converting each predicted tag indices back to tags\n",
    "tags_test = [tags[ind] for ind in tagids]\n",
    "print(\"Length of words in Padded test set:\",len(words_test))\n",
    "print(\"Length of tags in Padded test set:\",len(tags_test))\n",
    "print(\"\\nCheck few of words and predicted tags:\\n\",words_test[:10],tags_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main program (learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
